{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef194e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Recurrent layer\n",
    "        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers,\n",
    "                            dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # The output `X` shape: (`batch_size`, `num_steps`, `num_hiddens`)\n",
    "        # 这里因为Seq2Seq的输出是Decoder的输入，并没有经过最后的输出层，所以最后一个维度是num_hiddens\n",
    "        X = self.embedding(X)\n",
    "        X = X.permute(1, 0, 2) # permute(dims)将tensor的维度换位。\n",
    "        # 一般RNN输入就是时间步，批量大小，词表大小\n",
    "        # LSTM输入就是时间步，批量大小，词表大小\n",
    "        # RNN输出就是时间步，批量大小，隐藏单元大小\n",
    "        # LSTM输出就是时间步，批量大小，隐藏单元大小\n",
    "        output, (state_h, state_c) = self.rnn(X)\n",
    "        return output, (state_h, state_c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7, 4, 16])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval() #Dropout不会生效\n",
    "X = torch.zeros((4, 7), dtype=torch.long) # 4是batch_size, 7是句子的长度\n",
    "output, state = encoder(X)\n",
    "output.shape # torch.Size([7, 4, 16])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 4, 16])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2层，batch_size=4，num_hiddens=16\n",
    "state[0].shape # torch.Size([2, 4, 16])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        # 输出层\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1] # enc_outputs[1]是encoder的state\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        # num_layers = 2，这里拿的是第二层rnn的隐藏状态\n",
    "        \"\"\"\n",
    "         上下文操作。这里state[-1]拿到的是“最右上角的”H(这个H融合和所有的信息)如果state是【2，4，16】的，\n",
    "         那state[-1]就是【1,4,16】的。repeat重复时间步次。这样，每一个时间步都可以用到最后的H信息，\n",
    "         与新的输入X做concat操作（这也是为什么解码器的self.rnn是ebd_size + num_hiddens的原因）。\n",
    "         如果state[-1]是【1,4,16】，时间步是7，那重复完之后就是【7,4,16】的（7个时间步，4是batch_size，16是state隐藏单元的个数）。\n",
    "        \"\"\"\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1) # context的形状：(batch_size,num_steps,num_hiddens)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 7 but got size 14 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m decoder\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m      4\u001B[0m state \u001B[38;5;241m=\u001B[39m decoder\u001B[38;5;241m.\u001B[39minit_state(encoder(X))\n\u001B[0;32m----> 5\u001B[0m output, state \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m output\u001B[38;5;241m.\u001B[39mshape, state\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[5], line 27\u001B[0m, in \u001B[0;36mSeq2SeqDecoder.forward\u001B[0;34m(self, X, state)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m 上下文操作。这里state[-1]拿到的是“最右上角的”H(这个H融合和所有的信息)如果state是【2，4，16】的，\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124;03m 那state[-1]就是【1,4,16】的。repeat重复时间步次。这样，每一个时间步都可以用到最后的H信息，\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;03m 与新的输入X做concat操作（这也是为什么解码器的self.rnn是ebd_size + num_hiddens的原因）。\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m 如果state[-1]是【1,4,16】，时间步是7，那重复完之后就是【7,4,16】的（7个时间步，4是batch_size，16是state隐藏单元的个数）。\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     26\u001B[0m context \u001B[38;5;241m=\u001B[39m state[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mrepeat(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# context的形状：(batch_size,num_steps,num_hiddens)\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m X_and_context \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m output, state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(X_and_context, state)\n\u001B[1;32m     29\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(output)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Sizes of tensors must match except in dimension 2. Expected size 7 but got size 14 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
