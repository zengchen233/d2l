{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef194e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T02:22:23.579242Z",
     "start_time": "2023-10-19T02:22:22.317321Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5b346",
   "metadata": {},
   "source": [
    "`LSTM` 在每个时间步都有两个输出，一个是隐藏状态（通常是 `h_t`），另一个是细胞状态（通常是 `c_t`）\n",
    "\n",
    "`GRU` 在每个时间步只有一个输出，通常是隐藏状态（`h_t`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1754127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T02:22:23.582826Z",
     "start_time": "2023-10-19T02:22:23.580416Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Recurrent layer\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                            dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # The output `X` shape: (`batch_size`, `num_steps`, `num_hiddens`)\n",
    "        # 这里因为Seq2Seq的输出是Decoder的输入，并没有经过最后的输出层，所以最后一个维度是num_hiddens\n",
    "        X = self.embedding(X)\n",
    "        X = X.permute(1, 0, 2) # permute(dims)将tensor的维度换位。\n",
    "        # RNN, LSTM, GRU的output输出格式为:(num_steps, batch_size, num_hiddens)因为在nn.上述中没有进行输出，所以最后一个维度就是隐藏层大小\n",
    "        # 如果未提及状态，则默认为0\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        output, state = self.rnn(X)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa6efe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T02:22:23.587309Z",
     "start_time": "2023-10-19T02:22:23.583574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval() #Dropout不会生效\n",
    "X = torch.zeros((4, 7), dtype=torch.long) # 4是batch_size, 7是句子的长度\n",
    "output, state = encoder(X)\n",
    "output.shape # torch.Size([7, 4, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05f189b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T02:22:23.590545Z",
     "start_time": "2023-10-19T02:22:23.588447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2层，batch_size=4，num_hiddens=16\n",
    "state.shape # torch.Size([2, 4, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67958974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T02:22:23.594699Z",
     "start_time": "2023-10-19T02:22:23.591678Z"
    }
   },
   "outputs": [],
   "source": [
    "# 这里不太懂\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        # 输出层\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1] # enc_outputs[1]是encoder的state\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        # num_layers = 2，这里拿的是第二层rnn的隐藏状态\n",
    "        \"\"\"\n",
    "        上下文操作。这里state[-1]拿到的是“最右上角的”H(这个H融合和所有的信息)如果state是【2，4，16】的，\n",
    "        那state[-1]就是【1,4,16】的。repeat重复时间步次。这样，每一个时间步都可以用到最后的H信息，\n",
    "        与新的输入X做concat操作（这也是为什么解码器的self.rnn是ebd_size + num_hiddens的原因）。\n",
    "        如果state[-1]是【1,4,16】，时间步是7，那重复完之后就是【7,4,16】的（7个时间步，4是batch_size，16是state隐藏单元的个数）。\n",
    "        \"\"\"\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1) # context的形状：(num_steps * 1, batch_size, num_hiddens)\n",
    "        # 第1，2维不需要复制，在第0维复制num_steps次\n",
    "        X_and_context = torch.cat((X, context), 2) # 在第2维拼接\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2c86e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T02:22:23.599877Z",
     "start_time": "2023-10-19T02:22:23.595645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e5275",
   "metadata": {},
   "source": [
    "# 通过零值化来屏蔽不相关的项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a5d42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T02:22:23.604356Z",
     "start_time": "2023-10-19T02:22:23.600684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1) # 取第一维的大小\n",
    "    # None的作用主要是在使用None的位置新增一个维度。\n",
    "    A = torch.arange(maxlen, dtype=torch.float32, device=X.device)[None, :] # [[0, 1, 2]]\n",
    "    B = valid_len[:, None] # [[1],\n",
    "                           #  [2]]\n",
    "    # print(A.shape, B.shape)\n",
    "    mask =  A < B\n",
    "    \"\"\"\n",
    "    [\n",
    "        [T, F, F],\n",
    "        [T, T, F]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # ~表示将原来mask中为F的位置设置为0\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 【2，3】\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9baac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax的交叉熵损失函数\"\"\"\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
